{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing import load_data\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "from src.ngrams import *\n",
    "from src.string_similarity import levenshtein\n",
    "import operator\n",
    "from src.data_processing import print_progress\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from src.data_processing import load_cmu\n",
    "from src.ipatoarpabet import translate\n",
    "from string import punctuation\n",
    "from src.pronunciations import phonetic_distance\n",
    "import os\n",
    "from pattern.en import lexeme\n",
    "from src.pronunciations import get_closest_sounding_words as csw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model, this could take a while...\n"
     ]
    }
   ],
   "source": [
    "from src.pun_algorithms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/ngram_searchspace/ngram_totals2.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5993bf35ae1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/ngram_searchspace/ngram_totals2.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msearch_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/ngram_searchspace/ngram_totals2.json'"
     ]
    }
   ],
   "source": [
    "with open(\"data/ngram_searchspace/ngram_totals2.json\") as f:\n",
    "    search_space = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frequencies = defaultdict(int)\n",
    "for context in search_space:\n",
    "    for k, v, in context.items():\n",
    "        all_frequencies[k] = v['original_frequency']\n",
    "        for k1, v1 in v['substitutions'].items():\n",
    "            all_frequencies[k1] = v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1780"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(original_frequency, new_frequency, original_word, new_word, position, ph_penalty=2):\n",
    "#     return ((new_frequency - original_frequency) * phonetic_distance(original_word, new_word)) / position\n",
    "    return ((new_frequency - original_frequency)\n",
    "             * (phonetic_distance(original_word, new_word, translated=True) ** ph_penalty\n",
    "             * position)) # pos is normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_score(og_tri, new_tri, ph_penalty=2):\n",
    "    og_freq = all_frequencies[og_tri]\n",
    "    new_freq = all_frequencies[new_tri]\n",
    "    return score(og_freq, new_freq, og_tri.split()[1], new_tri.split()[1], 1, ph_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_accuracy(run):\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    \n",
    "    for i in range(len(task1)):\n",
    "        gold_pun = task1[i]['pun']\n",
    "        if run[i] and gold_pun:\n",
    "            tp += 1\n",
    "        if not run[i] and not gold_pun:\n",
    "            tn += 1\n",
    "        if run[i] and not gold_pun:\n",
    "            fp += 1\n",
    "        if not run[i] and gold_pun:\n",
    "            fn += 1\n",
    "    \n",
    "    results = {}\n",
    "    results['acc'] = (tp + tn) / len(task1)\n",
    "    results['recall'] = tp / (tp + fn)\n",
    "    results['prec'] = tp / (tp + fp)\n",
    "    results['f1'] = ((2*(results['recall'] * results['prec']))\n",
    "                      / (results['recall'] + results['prec']))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Trigram Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_trigram_baseline(index):\n",
    "    current_context = search_space[index]\n",
    "    for original_trigram in current_context:\n",
    "        if current_context[original_trigram]['original_frequency'] == 0:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [no_trigram_baseline(index) for index in range(len(search_space))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7825842696629214,\n",
       " 'f1': 0.8457552809884415,\n",
       " 'prec': 0.8570274636510501,\n",
       " 'recall': 0.8347757671125098}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_accuracy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4058"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_frequency('a staring contest'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Quadgram Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quadgrams_in_task1 = set()\n",
    "for context in task1:\n",
    "    words = context['words']\n",
    "    for i in range(len(words)-3):\n",
    "        all_quadgrams_in_task1.add(' '.join(words[i:i+4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quadgrams_in_task1 = list(sorted(all_quadgrams_in_task1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings = defaultdict(list)\n",
    "for quadgram in all_quadgrams_in_task1:\n",
    "    groupings[quadgram.split()[0][:3]].append(quadgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_to_dict(text):\n",
    "    ret = {}\n",
    "    for line in text.split('\\n'):\n",
    "        lsplit = line.split()\n",
    "        try:\n",
    "            ret[' '.join(lsplit[:4])] = int(lsplit[4])\n",
    "        except:\n",
    "            pass\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |████████████████████████████████████████████████████████████████████████████████████████████████████| 99.9% "
     ]
    }
   ],
   "source": [
    "quadgram_frequencies ={}\n",
    "for i, beginning_letters in enumerate(groupings):\n",
    "    try:\n",
    "        ngram_output = subprocess.check_output(['zcat', get_gram_file(beginning_letters, 4)]).decode('latin-1')\n",
    "    except:\n",
    "        pass\n",
    "    quad_dict = quad_to_dict(ngram_output)\n",
    "    for subquads in groupings[beginning_letters]:\n",
    "        try:\n",
    "#             print(subquads)\n",
    "            quadgram_frequencies[subquads] = quad_dict[subquads]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    print_progress(i, len(groupings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/quadgram_frequencies.json\") as f:\n",
    "    quadgram_frequencies = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_quadgram_baseline(index):\n",
    "    context = task1[index]['words']\n",
    "    for i in range(0, len(context)-3):\n",
    "        if ' '.join(context[i:i+4]) not in quadgram_frequencies:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [no_quadgram_baseline(i) for i in range(len(task1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7893258426966292,\n",
       " 'f1': 0.8700173310225303,\n",
       " 'prec': 0.7775712515489467,\n",
       " 'recall': 0.987411487018096}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_accuracy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frequencies = defaultdict(int)\n",
    "for context in search_space:\n",
    "    for og, sub in context.items():\n",
    "        all_frequencies[og] = sub['original_frequency']\n",
    "        for ssub, f in sub['substitutions'].items():\n",
    "            all_frequencies[ssub] = f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Trigrams, No Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(original_frequency, new_frequency, original_word, new_word, position, ph_penalty=2):\n",
    "    return ( (new_frequency - original_frequency)\n",
    "           * ((phonetic_distance(original_word, new_word, translated=True)**ph_penalty) \n",
    "           * position)) # pos is normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_score(original_trigram, new_trigram, ph_penalty=2):\n",
    "    original_freq = all_frequencies[original_trigram]\n",
    "    new_freq = all_frequencies[new_trigram]\n",
    "    og_word, new_word = original_trigram.split()[1], new_trigram.split()[1]\n",
    "    return score(original_freq, new_freq, og_word, new_word, 1, ph_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85163.12244897964"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_score('the orifice .', 'the area .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44444444444444453"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic_distance('orifice', 'office')**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_answers(unsorted_dict):\n",
    "    sd = {}\n",
    "    for k, d in unsorted_dict.items():\n",
    "        sd[k] = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_pos = {'ADV', 'ADJ', 'VERB', 'NOUN'}\n",
    "def rank_substitutions(index):\n",
    "    \n",
    "#     full_path = \"results/{}/{}\".format(path, index)\n",
    "    \n",
    "#     if os.path.exists(full_path):\n",
    "#         print(index)\n",
    "#         with open(full_path) as f:\n",
    "#             res = json.load(f)\n",
    "#         return res\n",
    "    \n",
    "    space = search_space[index]\n",
    "    context = task1[index]['words']\n",
    "    \n",
    "    # takes in list of subs, context is list of words\n",
    "    res = defaultdict(dict)\n",
    "    context_length = len(context)\n",
    "\n",
    "    for trigram, candidate in space.items():\n",
    "\n",
    "        # No Pos experiment, set to 1\n",
    "        position = context.index(trigram.split()[1])\n",
    "        end_position = context_length - position\n",
    "        \n",
    "        # take position and normalise it wrt length of context\n",
    "        if use_position:\n",
    "            normal_position = position / context_length\n",
    "        else:\n",
    "            normal_position = 1\n",
    "        \n",
    "        original_freq = candidate['original_frequency']     \n",
    "        original_word = trigram.split()[1]\n",
    "        \n",
    "        if original_word in cmu:\n",
    "            original_ph = cmu[original_word][0]\n",
    "        else:\n",
    "            # skip words not in new cmu\n",
    "            continue\n",
    "         \n",
    "        if use_filter:\n",
    "            phoneme_filter = set(csw(original_word))\n",
    "            \n",
    "        lexemes = lexeme(original_word)\n",
    "        \n",
    "        for sub, new_freq in candidate['substitutions'].items():\n",
    "            \n",
    "            new_word = sub.split()[1]\n",
    "            \n",
    "            \n",
    "            if use_filter:\n",
    "                if new_word not in phoneme_filter:\n",
    "                    continue\n",
    "            \n",
    "            # ignore lexical derivatives\n",
    "            if new_word in lexemes:\n",
    "                continue\n",
    "            \n",
    "            new_context = [w for w in context]\n",
    "            new_context[position-1:position+2] = sub.split()\n",
    "            \n",
    "            \n",
    "            if new_word in cmu:\n",
    "                new_ph = cmu[new_word][0]\n",
    "            else:\n",
    "                # skip words not in new cmu\n",
    "                continue\n",
    "            \n",
    "            if any([w in string.punctuation for w in new_word]):\n",
    "                continue\n",
    "                \n",
    "            tags = ([w[1] for w in \n",
    "                     pos_tag(new_context, tagset='universal')])\n",
    "            \n",
    "            if tags[position] not in accepted_pos:\n",
    "                continue\n",
    "\n",
    "            s = score(original_freq, \n",
    "                      new_freq, \n",
    "                      original_ph,\n",
    "                      new_ph,\n",
    "                      normal_position,\n",
    "                      ph_penalty=penalty)\n",
    "            \n",
    "            res[trigram][sub] = s\n",
    "            \n",
    "            \n",
    "        # might not need to write these to file, but write whole thing to file instead\n",
    "#     with open(full_path, 'w') as f:\n",
    "#         json.dump(sort_answers(res), f, indent=4)\n",
    "    \n",
    "    return sort_answers(res)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space[75]['its sedimental value']['substitutions']['its sentimental value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4f8446781fbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mngram_search_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank_substitutions\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbefore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         '''\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-7-75e6d0819334>\", line 72, in rank_substitutions\n",
      "    pos_tag(new_context, tagset='universal')])\n",
      "  File \"/home/doogy/.local/lib/python3.5/site-packages/nltk/tag/__init__.py\", line 134, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger)\n",
      "  File \"/home/doogy/.local/lib/python3.5/site-packages/nltk/tag/__init__.py\", line 102, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/home/doogy/.local/lib/python3.5/site-packages/nltk/tag/perceptron.py\", line 156, in tag\n",
      "    features = self._get_features(i, word, context, prev, prev2)\n",
      "  File \"/home/doogy/.local/lib/python3.5/site-packages/nltk/tag/perceptron.py\", line 268, in _get_features\n",
      "    add('i+1 suffix', context[i+1][-3:])\n",
      "  File \"/home/doogy/.local/lib/python3.5/site-packages/nltk/tag/perceptron.py\", line 250, in add\n",
      "    def add(name, *args):\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "use_position = True\n",
    "use_filter = False\n",
    "penalty=8\n",
    "\n",
    "before = time.time()\n",
    "p = Pool(4)\n",
    "ngram_search_space = p.map(rank_substitutions,   range(len(task1)))\n",
    "length = time.time() - before\n",
    "\n",
    "print(\"Total time taken in seconds: {}\".format(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/phonetic_filter_with_pos-ph-8-no-contractions\", 'w') as f:\n",
    "    json.dump(ngram_search_space, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use_position = True\n",
    "use_filter = True\n",
    "path = \"phonetic_filter_no_pos\"\n",
    "rank_substitutions(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonetic Generation, Running only on puns, no Tom Swiftys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Tom's from Data set, based solely on the word 'Tom'\n",
    "t1_no_toms, no_toms_search_space = [], []\n",
    "for i, p in enumerate(task1):\n",
    "    if 'Tom' not in p['words']:\n",
    "        t1_no_toms.append(p)\n",
    "        no_toms_search_space.append(search_space[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_score(distance, frequency_difference, position):\n",
    "    return frequency_difference / ((distance**2 + 1 + position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_toms_search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "for i, results in enumerate(no_toms_search_space):\n",
    "    if results == 'miss':\n",
    "        all_res.append(('miss', 0))\n",
    "        continue\n",
    "    max_score = 0\n",
    "    max_word = ''\n",
    "    for original_word, replacements in results.items():\n",
    "        pos = t1_no_toms[i]['words'].index(original_word)\n",
    "        pos = len(t1_no_toms[i]['words']) - pos\n",
    "        for subs in replacements:\n",
    "            score = switch_score(subs[1], subs[2], pos)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_word = subs[0]\n",
    "    all_res.append((max_word, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"\"\"0\n",
    "1\n",
    "112\n",
    "2\n",
    "113\n",
    "224\n",
    "3\n",
    "225\n",
    "4\n",
    "226\n",
    "114\n",
    "5\n",
    "6\n",
    "115\n",
    "7\n",
    "227\n",
    "116\n",
    "8\n",
    "117\n",
    "118\n",
    "228\n",
    "9\n",
    "229\n",
    "10\n",
    "11\n",
    "119\n",
    "230\n",
    "231\n",
    "232\n",
    "233\n",
    "234\n",
    "235\n",
    "236\n",
    "237\n",
    "238\n",
    "239\n",
    "240\n",
    "241\n",
    "242\n",
    "243\n",
    "244\n",
    "245\n",
    "246\n",
    "247\n",
    "248\n",
    "249\n",
    "250\n",
    "251\n",
    "252\n",
    "253\n",
    "254\n",
    "255\n",
    "256\n",
    "257\"\"\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "for i, result in enumerate(all_res):\n",
    "    if result[1] > 10:\n",
    "        if t1_no_toms[i]['pun']:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if not t1_no_toms[i]['pun']:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "            \n",
    "acc = (tp + tn) / len(all_res)\n",
    "prec = tp / (tp + fn)\n",
    "recall = tp / (tp + fp)\n",
    "f1 = (2*(recall*prec)) / (recall + prec)\n",
    "#     accuracies.append(acc)\n",
    "#     precisions.append(prec)\n",
    "#     recalls.append(recall)\n",
    "#     f1s.append(f1)\n",
    "print(tp, tn, fp, fn)\n",
    "print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1: {}\".format(acc, prec, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Phonetic Generation, Tom Detection + Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models.KeyedVectors.load_word2vec_format('/home/doogy/Data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem('babies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "for i, results in enumerate(search_space):\n",
    "    print_progress(i+1, len(search_space))\n",
    "    if is_Tom_Swifty(' '.join(task1[i]['words']), m):\n",
    "        all_res.append(('tom', 1))\n",
    "        continue\n",
    "    if results == 'miss':\n",
    "        all_res.append(('miss', 0))\n",
    "        continue\n",
    "    max_score = 0\n",
    "    max_word = ''\n",
    "    for original_word, replacements in results.items():\n",
    "        og_stem = stemmer.stem(original_word)\n",
    "        pos = task1[i]['words'].index(original_word)\n",
    "        pos = len(task1[i]['words']) - pos\n",
    "        for subs in replacements:\n",
    "            if stemmer.stem(subs[0]) == og_stem:\n",
    "                continue\n",
    "            score = switch_score(subs[1], subs[2], 0)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_word = subs[0]\n",
    "    all_res.append((max_word, max_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, res in enumerate(all_res):\n",
    "    print(i, res, ' '.join(task1[i]['words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space[1762]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_closest_sounding_words('ordure'), cmu['order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_word, subs in baby_oil.items():\n",
    "    max_word, max_score = '', 0\n",
    "    print(original_word, subs)\n",
    "    pos = len(task1[2]['words']) - task1[2]['words'].index(original_word)\n",
    "    for sub in subs:\n",
    "        print(switch_score(sub[1], sub[2], pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "for i, result in enumerate(all_res):\n",
    "    if result[1] > 0:\n",
    "        if task1[i]['pun']:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if not task1[i]['pun']:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "            \n",
    "acc = (tp + tn) / len(all_res)\n",
    "prec = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = (2*(recall*prec)) / (recall + prec)\n",
    "#     accuracies.append(acc)\n",
    "#     precisions.append(prec)\n",
    "#     recalls.append(recall)\n",
    "#     f1s.append(f1)\n",
    "print(tp, tn, fp, fn)\n",
    "print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1: {}\".format(acc, prec, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.similarity('ledge', 'mountain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(search_space)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
