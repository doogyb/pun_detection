{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing import load_data\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "from src.ngrams import *\n",
    "from src.string_similarity import levenshtein\n",
    "import operator\n",
    "from src.data_processing import print_progress\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from src.data_processing import load_cmu\n",
    "from src.ipatoarpabet import translate\n",
    "from string import punctuation\n",
    "from src.pronunciations import phonetic_distance\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model, this could take a while...\n"
     ]
    }
   ],
   "source": [
    "from src.pun_algorithms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/ngram_searchspace/ngram_totals.json\") as f:\n",
    "    search_space = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1, task2, task3, min_pairs, strings, pun_strings = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_accuracy(run):\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    \n",
    "    for i in range(len(task1)):\n",
    "        gold_pun = task1[i]['pun']\n",
    "        if run[i] and gold_pun:\n",
    "            tp += 1\n",
    "        if not run[i] and not gold_pun:\n",
    "            tn += 1\n",
    "        if run[i] and not gold_pun:\n",
    "            fp += 1\n",
    "        if not run[i] and gold_pun:\n",
    "            fn += 1\n",
    "    \n",
    "    results = {}\n",
    "    results['acc'] = (tp + tn) / len(task1)\n",
    "    results['recall'] = tp / (tp + fn)\n",
    "    results['prec'] = tp / (tp + fp)\n",
    "    results['f1'] = ((2*(results['recall'] * results['prec']))\n",
    "                      / (results['recall'] + results['prec']))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Trigram Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_trigram_baseline(index):\n",
    "    current_context = search_space[index]\n",
    "    for original_trigram in current_context:\n",
    "        if current_context[original_trigram]['original_frequency'] == 0:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [no_trigram_baseline(index) for index in range(len(search_space))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7792134831460674,\n",
       " 'f1': 0.841596130592503,\n",
       " 'prec': 0.8628099173553719,\n",
       " 'recall': 0.8214004720692368}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_accuracy(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Quadgram Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quadgrams_in_task1 = set()\n",
    "for context in task1:\n",
    "    words = context['words']\n",
    "    for i in range(len(words)-3):\n",
    "        all_quadgrams_in_task1.add(' '.join(words[i:i+4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quadgrams_in_task1 = list(sorted(all_quadgrams_in_task1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings = defaultdict(list)\n",
    "for quadgram in all_quadgrams_in_task1:\n",
    "    groupings[quadgram.split()[0][:3]].append(quadgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_to_dict(text):\n",
    "    ret = {}\n",
    "    for line in text.split('\\n'):\n",
    "        lsplit = line.split()\n",
    "        try:\n",
    "            ret[' '.join(lsplit[:4])] = int(lsplit[4])\n",
    "        except:\n",
    "            pass\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |████████████████████████████████████████████████████████████████████████████████████████████████████| 99.9% "
     ]
    }
   ],
   "source": [
    "quadgram_frequencies ={}\n",
    "for i, beginning_letters in enumerate(groupings):\n",
    "    try:\n",
    "        ngram_output = subprocess.check_output(['zcat', get_gram_file(beginning_letters, 4)]).decode('latin-1')\n",
    "    except:\n",
    "        pass\n",
    "    quad_dict = quad_to_dict(ngram_output)\n",
    "    for subquads in groupings[beginning_letters]:\n",
    "        try:\n",
    "#             print(subquads)\n",
    "            quadgram_frequencies[subquads] = quad_dict[subquads]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    print_progress(i, len(groupings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/quadgram_frequencies.json\") as f:\n",
    "    quadgram_frequencies = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_quadgram_baseline(index):\n",
    "    context = task1[index]['words']\n",
    "    for i in range(0, len(context)-3):\n",
    "        if ' '.join(context[i:i+4]) not in quadgram_frequencies:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [no_quadgram_baseline(i) for i in range(len(task1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7893258426966292,\n",
       " 'f1': 0.8700173310225303,\n",
       " 'prec': 0.7775712515489467,\n",
       " 'recall': 0.987411487018096}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_accuracy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1188118"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Trigrams, No Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(original_frequency, new_frequency, original_word, new_word, position):\n",
    "    return ( (new_frequency - original_frequency)\n",
    "           * ((phonetic_distance(original_word, new_word, translated=True)**2) \n",
    "           * position)) # pos is normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_answers(unsorted_dict):\n",
    "    sd = {}\n",
    "    for k, d in unsorted_dict.items():\n",
    "        sd[k] = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_pos = {'ADV', 'ADJ', 'VERB', 'NOUN'}\n",
    "def rank_substitutions(index):\n",
    "    \n",
    "    \n",
    "    if os.path.exists(\"results/{}/{}\".format(path, index)):\n",
    "        print(index)\n",
    "        return {}\n",
    "    \n",
    "    space = search_space[index]\n",
    "    context = task1[index]['words']\n",
    "    \n",
    "    # takes in list of subs, context is list of words\n",
    "    res = defaultdict(dict)\n",
    "    context_length = len(context)\n",
    "\n",
    "    for trigram, candidate in space.items():\n",
    "\n",
    "        # No Pos experiment, set to 1\n",
    "        position = context.index(trigram.split()[1])\n",
    "        end_position = context_length - position\n",
    "        \n",
    "        # take position and normalise it wrt length of context\n",
    "        if use_position:\n",
    "            normal_position = position / context_length\n",
    "        else:\n",
    "            normal_position = 1\n",
    "        \n",
    "        original_freq = candidate['original_frequency']\n",
    "        \n",
    "        original_word = trigram.split()[1]\n",
    "        if original_word in cmu:\n",
    "            original_ph = cmu[original_word][0]\n",
    "        else:\n",
    "            # skip words not in new cmu\n",
    "            continue\n",
    "        \n",
    "        for sub, new_freq in candidate['substitutions'].items():\n",
    "            \n",
    "            new_context = [w for w in context]\n",
    "            new_context[position-1:position+2] = sub.split()\n",
    "            \n",
    "            new_word = sub.split()[1]\n",
    "            if new_word in cmu:\n",
    "                new_ph = cmu[new_word][0]\n",
    "            else:\n",
    "                # skip words not in new cmu\n",
    "                continue\n",
    "            \n",
    "            if any([w in string.punctuation for w in new_word]):\n",
    "                continue\n",
    "                \n",
    "            tags = [w[1] for w in pos_tag(new_context, tagset='universal')]\n",
    "            \n",
    "            if tags[position] not in accepted_pos:\n",
    "                continue\n",
    "\n",
    "            s = score(original_freq, \n",
    "                      new_freq, \n",
    "                      original_ph,\n",
    "                      new_ph,\n",
    "                      normal_position)\n",
    "            \n",
    "            res[trigram][sub] = s\n",
    "            \n",
    "            \n",
    "\n",
    "    with open(\"results/all_trigram_with_pos/{}\".format(index), 'w') as f:\n",
    "        json.dump(sort_answers(res), f, indent=4)\n",
    "    \n",
    "    return res        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'use_position' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-9-2fd19cc4364a>\", line 23, in rank_substitutions\n    if use_position:\nNameError: name 'use_position' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-53cf42a0cd01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mngram_search_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank_substitutions\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbefore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total time taken in seconds: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         '''\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'use_position' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"phonetic_filter_no_pos\"\n",
    "use_pos = False\n",
    "import time\n",
    "before = time.time()\n",
    "from multiprocessing import Pool\n",
    "p = Pool(4)\n",
    "ngram_search_space = p.map(rank_substitutions,   range(len(search_space)))\n",
    "length = time.time() - before\n",
    "print(\"Total time taken in seconds: {}\".format(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AO/F/AH/S\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['src/c/closest_sounding_words', 'office', 'AO/F/AH/S', '0']' returned non-zero exit status 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5c35f58fdfa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpronunciations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_closest_sounding_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_closest_sounding_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'office'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/pun_detection/src/pronunciations.py\u001b[0m in \u001b[0;36mget_closest_sounding_words\u001b[0;34m(in_word, cthreshold, share_first_letter, use_dictionary)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mpros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"src/c/closest_sounding_words\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mpros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpros\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_dictionary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 626\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 708\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['src/c/closest_sounding_words', 'office', 'AO/F/AH/S', '0']' returned non-zero exit status 1"
     ]
    }
   ],
   "source": [
    "from src.pronunciations import get_closest_sounding_words\n",
    "get_closest_sounding_words('office')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonetic Generation, Running only on puns, no Tom Swiftys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Tom's from Data set, based solely on the word 'Tom'\n",
    "t1_no_toms, no_toms_search_space = [], []\n",
    "for i, p in enumerate(task1):\n",
    "    if 'Tom' not in p['words']:\n",
    "        t1_no_toms.append(p)\n",
    "        no_toms_search_space.append(search_space[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_score(distance, frequency_difference, position):\n",
    "    return frequency_difference / ((distance**2 + 1 + position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_toms_search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "for i, results in enumerate(no_toms_search_space):\n",
    "    if results == 'miss':\n",
    "        all_res.append(('miss', 0))\n",
    "        continue\n",
    "    max_score = 0\n",
    "    max_word = ''\n",
    "    for original_word, replacements in results.items():\n",
    "        pos = t1_no_toms[i]['words'].index(original_word)\n",
    "        pos = len(t1_no_toms[i]['words']) - pos\n",
    "        for subs in replacements:\n",
    "            score = switch_score(subs[1], subs[2], pos)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_word = subs[0]\n",
    "    all_res.append((max_word, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"\"\"0\n",
    "1\n",
    "112\n",
    "2\n",
    "113\n",
    "224\n",
    "3\n",
    "225\n",
    "4\n",
    "226\n",
    "114\n",
    "5\n",
    "6\n",
    "115\n",
    "7\n",
    "227\n",
    "116\n",
    "8\n",
    "117\n",
    "118\n",
    "228\n",
    "9\n",
    "229\n",
    "10\n",
    "11\n",
    "119\n",
    "230\n",
    "231\n",
    "232\n",
    "233\n",
    "234\n",
    "235\n",
    "236\n",
    "237\n",
    "238\n",
    "239\n",
    "240\n",
    "241\n",
    "242\n",
    "243\n",
    "244\n",
    "245\n",
    "246\n",
    "247\n",
    "248\n",
    "249\n",
    "250\n",
    "251\n",
    "252\n",
    "253\n",
    "254\n",
    "255\n",
    "256\n",
    "257\"\"\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "for i, result in enumerate(all_res):\n",
    "    if result[1] > 10:\n",
    "        if t1_no_toms[i]['pun']:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if not t1_no_toms[i]['pun']:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "            \n",
    "acc = (tp + tn) / len(all_res)\n",
    "prec = tp / (tp + fn)\n",
    "recall = tp / (tp + fp)\n",
    "f1 = (2*(recall*prec)) / (recall + prec)\n",
    "#     accuracies.append(acc)\n",
    "#     precisions.append(prec)\n",
    "#     recalls.append(recall)\n",
    "#     f1s.append(f1)\n",
    "print(tp, tn, fp, fn)\n",
    "print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1: {}\".format(acc, prec, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Phonetic Generation, Tom Detection + Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models.KeyedVectors.load_word2vec_format('/home/doogy/Data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem('babies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "for i, results in enumerate(search_space):\n",
    "    print_progress(i+1, len(search_space))\n",
    "    if is_Tom_Swifty(' '.join(task1[i]['words']), m):\n",
    "        all_res.append(('tom', 1))\n",
    "        continue\n",
    "    if results == 'miss':\n",
    "        all_res.append(('miss', 0))\n",
    "        continue\n",
    "    max_score = 0\n",
    "    max_word = ''\n",
    "    for original_word, replacements in results.items():\n",
    "        og_stem = stemmer.stem(original_word)\n",
    "        pos = task1[i]['words'].index(original_word)\n",
    "        pos = len(task1[i]['words']) - pos\n",
    "        for subs in replacements:\n",
    "            if stemmer.stem(subs[0]) == og_stem:\n",
    "                continue\n",
    "            score = switch_score(subs[1], subs[2], 0)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_word = subs[0]\n",
    "    all_res.append((max_word, max_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, res in enumerate(all_res):\n",
    "    print(i, res, ' '.join(task1[i]['words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space[1762]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_closest_sounding_words('ordure'), cmu['order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_word, subs in baby_oil.items():\n",
    "    max_word, max_score = '', 0\n",
    "    print(original_word, subs)\n",
    "    pos = len(task1[2]['words']) - task1[2]['words'].index(original_word)\n",
    "    for sub in subs:\n",
    "        print(switch_score(sub[1], sub[2], pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "for i, result in enumerate(all_res):\n",
    "    if result[1] > 0:\n",
    "        if task1[i]['pun']:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if not task1[i]['pun']:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "            \n",
    "acc = (tp + tn) / len(all_res)\n",
    "prec = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = (2*(recall*prec)) / (recall + prec)\n",
    "#     accuracies.append(acc)\n",
    "#     precisions.append(prec)\n",
    "#     recalls.append(recall)\n",
    "#     f1s.append(f1)\n",
    "print(tp, tn, fp, fn)\n",
    "print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1: {}\".format(acc, prec, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.similarity('ledge', 'mountain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(search_space)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
