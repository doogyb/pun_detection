{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing import load_data\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "from src.ngrams import *\n",
    "from src.string_similarity import levenshtein\n",
    "import operator\n",
    "from src.data_processing import print_progress\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from src.data_processing import load_cmu\n",
    "from src.ipatoarpabet import translate\n",
    "from string import punctuation\n",
    "from src.pronunciations import phonetic_distance\n",
    "import os\n",
    "from pattern.en import lexeme\n",
    "from src.pronunciations import get_closest_sounding_words as csw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model, this could take a while...\n"
     ]
    }
   ],
   "source": [
    "from src.pun_algorithms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/ngram_searchspace/ngram_totals2.json\") as f:\n",
    "    search_space = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1780"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(66639115, 7263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1, task2, task3, min_pairs, strings, pun_strings = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_accuracy(run):\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    \n",
    "    for i in range(len(task1)):\n",
    "        gold_pun = task1[i]['pun']\n",
    "        if run[i] and gold_pun:\n",
    "            tp += 1\n",
    "        if not run[i] and not gold_pun:\n",
    "            tn += 1\n",
    "        if run[i] and not gold_pun:\n",
    "            fp += 1\n",
    "        if not run[i] and gold_pun:\n",
    "            fn += 1\n",
    "    \n",
    "    results = {}\n",
    "    results['acc'] = (tp + tn) / len(task1)\n",
    "    results['recall'] = tp / (tp + fn)\n",
    "    results['prec'] = tp / (tp + fp)\n",
    "    results['f1'] = ((2*(results['recall'] * results['prec']))\n",
    "                      / (results['recall'] + results['prec']))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Trigram Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_trigram_baseline(index):\n",
    "    current_context = search_space[index]\n",
    "    for original_trigram in current_context:\n",
    "        if current_context[original_trigram]['original_frequency'] == 0:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [no_trigram_baseline(index) for index in range(len(search_space))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7825842696629214,\n",
       " 'f1': 0.8457552809884415,\n",
       " 'prec': 0.8570274636510501,\n",
       " 'recall': 0.8347757671125098}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_accuracy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4058"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_frequency('a staring contest'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Quadgram Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quadgrams_in_task1 = set()\n",
    "for context in task1:\n",
    "    words = context['words']\n",
    "    for i in range(len(words)-3):\n",
    "        all_quadgrams_in_task1.add(' '.join(words[i:i+4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quadgrams_in_task1 = list(sorted(all_quadgrams_in_task1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings = defaultdict(list)\n",
    "for quadgram in all_quadgrams_in_task1:\n",
    "    groupings[quadgram.split()[0][:3]].append(quadgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_to_dict(text):\n",
    "    ret = {}\n",
    "    for line in text.split('\\n'):\n",
    "        lsplit = line.split()\n",
    "        try:\n",
    "            ret[' '.join(lsplit[:4])] = int(lsplit[4])\n",
    "        except:\n",
    "            pass\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |████████████████████████████████████████████████████████████████████████████████████████████████████| 99.9% "
     ]
    }
   ],
   "source": [
    "quadgram_frequencies ={}\n",
    "for i, beginning_letters in enumerate(groupings):\n",
    "    try:\n",
    "        ngram_output = subprocess.check_output(['zcat', get_gram_file(beginning_letters, 4)]).decode('latin-1')\n",
    "    except:\n",
    "        pass\n",
    "    quad_dict = quad_to_dict(ngram_output)\n",
    "    for subquads in groupings[beginning_letters]:\n",
    "        try:\n",
    "#             print(subquads)\n",
    "            quadgram_frequencies[subquads] = quad_dict[subquads]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    print_progress(i, len(groupings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/quadgram_frequencies.json\") as f:\n",
    "    quadgram_frequencies = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_quadgram_baseline(index):\n",
    "    context = task1[index]['words']\n",
    "    for i in range(0, len(context)-3):\n",
    "        if ' '.join(context[i:i+4]) not in quadgram_frequencies:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [no_quadgram_baseline(i) for i in range(len(task1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7893258426966292,\n",
       " 'f1': 0.8700173310225303,\n",
       " 'prec': 0.7775712515489467,\n",
       " 'recall': 0.987411487018096}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_accuracy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frequencies = defaultdict(int)\n",
    "for context in search_space:\n",
    "    for og, sub in context.items():\n",
    "        all_frequencies[og] = sub['original_frequency']\n",
    "        for ssub, f in sub['substitutions'].items():\n",
    "            all_frequencies[ssub] = f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Trigrams, No Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(original_frequency, new_frequency, original_word, new_word, position, ph_penalty=2):\n",
    "    return ( (new_frequency - original_frequency)\n",
    "           * ((phonetic_distance(original_word, new_word, translated=True)**ph_penalty) \n",
    "           * position)) # pos is normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_score(original_trigram, new_trigram, ph_penalty=2):\n",
    "    original_freq = all_frequencies[original_trigram]\n",
    "    new_freq = all_frequencies[new_trigram]\n",
    "    og_word, new_word = original_trigram.split()[1], new_trigram.split()[1]\n",
    "    return score(original_freq, new_freq, og_word, new_word, 1, ph_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_answers(unsorted_dict):\n",
    "    sd = {}\n",
    "    for k, d in unsorted_dict.items():\n",
    "        sd[k] = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_pos = {'ADV', 'ADJ', 'VERB', 'NOUN'}\n",
    "def rank_substitutions(index):\n",
    "    \n",
    "#     full_path = \"results/{}/{}\".format(path, index)\n",
    "    \n",
    "#     if os.path.exists(full_path):\n",
    "#         print(index)\n",
    "#         with open(full_path) as f:\n",
    "#             res = json.load(f)\n",
    "#         return res\n",
    "    \n",
    "    space = search_space[index]\n",
    "    context = task1[index]['words']\n",
    "    \n",
    "    # takes in list of subs, context is list of words\n",
    "    res = defaultdict(dict)\n",
    "    context_length = len(context)\n",
    "\n",
    "    for trigram, candidate in space.items():\n",
    "\n",
    "        # No Pos experiment, set to 1\n",
    "        position = context.index(trigram.split()[1])\n",
    "        end_position = context_length - position\n",
    "        \n",
    "        # take position and normalise it wrt length of context\n",
    "        if use_position:\n",
    "            normal_position = position / context_length\n",
    "        else:\n",
    "            normal_position = 1\n",
    "        \n",
    "        original_freq = candidate['original_frequency']     \n",
    "        original_word = trigram.split()[1]\n",
    "        \n",
    "        if original_word in cmu:\n",
    "            original_ph = cmu[original_word][0]\n",
    "        else:\n",
    "            # skip words not in new cmu\n",
    "            continue\n",
    "         \n",
    "        if use_filter:\n",
    "            phoneme_filter = set(csw(original_word))\n",
    "            \n",
    "        lexemes = lexeme(original_word)\n",
    "        \n",
    "        for sub, new_freq in candidate['substitutions'].items():\n",
    "            \n",
    "            new_word = sub.split()[1]\n",
    "            \n",
    "            \n",
    "            if use_filter:\n",
    "                if new_word not in phoneme_filter:\n",
    "                    continue\n",
    "            \n",
    "            # ignore lexical derivatives\n",
    "            if new_word in lexemes:\n",
    "                continue\n",
    "            \n",
    "            new_context = [w for w in context]\n",
    "            new_context[position-1:position+2] = sub.split()\n",
    "            \n",
    "            \n",
    "            if new_word in cmu:\n",
    "                new_ph = cmu[new_word][0]\n",
    "            else:\n",
    "                # skip words not in new cmu\n",
    "                continue\n",
    "            \n",
    "            if any([w in string.punctuation for w in new_word]):\n",
    "                continue\n",
    "                \n",
    "            tags = ([w[1] for w in \n",
    "                     pos_tag(new_context, tagset='universal')])\n",
    "            \n",
    "            if tags[position] not in accepted_pos:\n",
    "                continue\n",
    "\n",
    "            s = score(original_freq, \n",
    "                      new_freq, \n",
    "                      original_ph,\n",
    "                      new_ph,\n",
    "                      normal_position,\n",
    "                      ph_penalty=penalty)\n",
    "            \n",
    "            res[trigram][sub] = s\n",
    "            \n",
    "            \n",
    "        # might not need to write these to file, but write whole thing to file instead\n",
    "#     with open(full_path, 'w') as f:\n",
    "#         json.dump(sort_answers(res), f, indent=4)\n",
    "    \n",
    "    return sort_answers(res)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "use_position = True\n",
    "use_filter = False\n",
    "penalty=8\n",
    "\n",
    "before = time.time()\n",
    "p = Pool(4)\n",
    "ngram_search_space = p.map(rank_substitutions,   range(len(task1)))\n",
    "length = time.time() - before\n",
    "\n",
    "print(\"Total time taken in seconds: {}\".format(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/phonetic_filter_with_pos-ph-8-no-contractions\", 'w') as f:\n",
    "    json.dump(ngram_search_space, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use_position = True\n",
    "use_filter = True\n",
    "path = \"phonetic_filter_no_pos\"\n",
    "rank_substitutions(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonetic Generation, Running only on puns, no Tom Swiftys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Tom's from Data set, based solely on the word 'Tom'\n",
    "t1_no_toms, no_toms_search_space = [], []\n",
    "for i, p in enumerate(task1):\n",
    "    if 'Tom' not in p['words']:\n",
    "        t1_no_toms.append(p)\n",
    "        no_toms_search_space.append(search_space[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_score(distance, frequency_difference, position):\n",
    "    return frequency_difference / ((distance**2 + 1 + position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_toms_search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "for i, results in enumerate(no_toms_search_space):\n",
    "    if results == 'miss':\n",
    "        all_res.append(('miss', 0))\n",
    "        continue\n",
    "    max_score = 0\n",
    "    max_word = ''\n",
    "    for original_word, replacements in results.items():\n",
    "        pos = t1_no_toms[i]['words'].index(original_word)\n",
    "        pos = len(t1_no_toms[i]['words']) - pos\n",
    "        for subs in replacements:\n",
    "            score = switch_score(subs[1], subs[2], pos)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_word = subs[0]\n",
    "    all_res.append((max_word, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "for i, result in enumerate(all_res):\n",
    "    if result[1] > 10:\n",
    "        if t1_no_toms[i]['pun']:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if not t1_no_toms[i]['pun']:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "            \n",
    "acc = (tp + tn) / len(all_res)\n",
    "prec = tp / (tp + fn)\n",
    "recall = tp / (tp + fp)\n",
    "f1 = (2*(recall*prec)) / (recall + prec)\n",
    "#     accuracies.append(acc)\n",
    "#     precisions.append(prec)\n",
    "#     recalls.append(recall)\n",
    "#     f1s.append(f1)\n",
    "print(tp, tn, fp, fn)\n",
    "print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1: {}\".format(acc, prec, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Phonetic Generation, Tom Detection + Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models.KeyedVectors.load_word2vec_format('/home/doogy/Data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem('babies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "for i, results in enumerate(search_space):\n",
    "    print_progress(i+1, len(search_space))\n",
    "    if is_Tom_Swifty(' '.join(task1[i]['words']), m):\n",
    "        all_res.append(('tom', 1))\n",
    "        continue\n",
    "    if results == 'miss':\n",
    "        all_res.append(('miss', 0))\n",
    "        continue\n",
    "    max_score = 0\n",
    "    max_word = ''\n",
    "    for original_word, replacements in results.items():\n",
    "        og_stem = stemmer.stem(original_word)\n",
    "        pos = task1[i]['words'].index(original_word)\n",
    "        pos = len(task1[i]['words']) - pos\n",
    "        for subs in replacements:\n",
    "            if stemmer.stem(subs[0]) == og_stem:\n",
    "                continue\n",
    "            score = switch_score(subs[1], subs[2], 0)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_word = subs[0]\n",
    "    all_res.append((max_word, max_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, res in enumerate(all_res):\n",
    "    print(i, res, ' '.join(task1[i]['words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space[1762]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_closest_sounding_words('ordure'), cmu['order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_word, subs in baby_oil.items():\n",
    "    max_word, max_score = '', 0\n",
    "    print(original_word, subs)\n",
    "    pos = len(task1[2]['words']) - task1[2]['words'].index(original_word)\n",
    "    for sub in subs:\n",
    "        print(switch_score(sub[1], sub[2], pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "for i, result in enumerate(all_res):\n",
    "    if result[1] > 0:\n",
    "        if task1[i]['pun']:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if not task1[i]['pun']:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "            \n",
    "acc = (tp + tn) / len(all_res)\n",
    "prec = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = (2*(recall*prec)) / (recall + prec)\n",
    "#     accuracies.append(acc)\n",
    "#     precisions.append(prec)\n",
    "#     recalls.append(recall)\n",
    "#     f1s.append(f1)\n",
    "print(tp, tn, fp, fn)\n",
    "print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1: {}\".format(acc, prec, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.similarity('ledge', 'mountain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(search_space)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
